{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "\n",
    "def get_esm_embeddings(sequences):\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()\n",
    "\n",
    "    data = [('protein'+str(i), seq) for i, seq in enumerate(sequences)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "\n",
    "    token_embeddings = results[\"representations\"][33] \n",
    "\n",
    "    sequence_embeddings = []\n",
    "    for i in range(len(sequences)):\n",
    "        seq_len = len(sequences[i])\n",
    "        seq_emb = token_embeddings[i, 1:1+seq_len]\n",
    "        sequence_embeddings.append(seq_emb.cpu().numpy())\n",
    "    return sequence_embeddings\n",
    "\n",
    "def embed_and_pool(sequences):\n",
    "    seq_emb_list = get_esm_embeddings(sequences)\n",
    "    pooled_list = []\n",
    "    for emb in seq_emb_list:\n",
    "        pooled_vec = emb.mean(axis=0)  \n",
    "        pooled_list.append(pooled_vec)\n",
    "    return np.array(pooled_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X.float()\n",
    "        \n",
    "        self.y = None\n",
    "        if y is not None:\n",
    "            if isinstance(y, np.ndarray):\n",
    "                self.y = torch.from_numpy(y).long()  \n",
    "            else:\n",
    "                self.y = y.long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512,256,128,64], num_classes=2):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, num_classes)) \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def cross_val_score_mlp(X, y, \n",
    "                        hidden_dims,\n",
    "                        lr,\n",
    "                        batch_size,\n",
    "                        device,\n",
    "                        n_splits=5,\n",
    "                        max_epochs=30,\n",
    "                        patience=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_dataset = ProteinDataset(X_train, y_train)\n",
    "        val_dataset = ProteinDataset(X_val, y_val)\n",
    "        \n",
    "        class_sample_counts = np.bincount(y_train)  \n",
    "        weights_per_class = 1.0 / (class_sample_counts + 1e-8)\n",
    "        samples_weight = weights_per_class[y_train]  \n",
    "        \n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=samples_weight, \n",
    "            num_samples=len(samples_weight), \n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        input_dim = X.shape[1]\n",
    "        model = MLP(input_dim, hidden_dims=hidden_dims, num_classes=2).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss() \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        best_auc = 0.0\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_probs = []\n",
    "            val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    outputs = model(batch_x)\n",
    "                    probs = torch.softmax(outputs, dim=1)[:,1]  \n",
    "                    val_probs.extend(probs.cpu().numpy())\n",
    "                    val_targets.extend(batch_y.numpy())\n",
    "            \n",
    "            val_auc = roc_auc_score(val_targets, val_probs)\n",
    "            \n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_state = model.state_dict()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "        \n",
    "        auc_scores.append(best_auc)\n",
    "\n",
    "    return np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dims = []\n",
    "    for i in range(4):\n",
    "        hidden_dim = trial.suggest_int(f\"hidden_dim_{i}\", 64, 512, step=64)\n",
    "        hidden_dims.append(hidden_dim)\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    \n",
    "    mean_auc = cross_val_score_mlp(\n",
    "        X_trainval, y_trainval,\n",
    "        hidden_dims=hidden_dims,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        n_splits=5,\n",
    "        max_epochs=30,   \n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    return mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "def predict_on_csv_in_chunks(csv_file, model_file, device, output_csv=\"new_data_with_pred.csv\", chunk_size=100):\n",
    "\n",
    "    df_new = pd.read_csv(csv_file)\n",
    "    seqs_new = df_new[\"orf_sequence\"].tolist()\n",
    "    model = MLP(input_dim=1280, hidden_dims=[128,192,192,64], num_classes=2).to(device)\n",
    "    model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "\n",
    "    num_seqs = len(seqs_new)\n",
    "    num_chunks = math.ceil(num_seqs / chunk_size)\n",
    "    \n",
    "    print(f\"Total sequences: {num_seqs}. We will process {num_chunks} chunks (chunk_size={chunk_size}).\")\n",
    "    \n",
    "    idx_start = 0\n",
    "    for chunk_i in range(num_chunks):\n",
    "        idx_end = min(idx_start + chunk_size, num_seqs)\n",
    "        sub_seqs = seqs_new[idx_start : idx_end]\n",
    "        \n",
    "        X_new_sub = embed_and_pool(sub_seqs)  \n",
    "        X_tensor = torch.from_numpy(X_new_sub).float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)                       \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]     \n",
    "            all_probs_sub = probs.cpu().numpy().tolist()    \n",
    "        \n",
    "        all_probs.extend(all_probs_sub)\n",
    "        \n",
    "        idx_start = idx_end\n",
    "        print(f\"Chunk {chunk_i+1}/{num_chunks} done. Current total predictions={len(all_probs)}\")\n",
    "    \n",
    "\n",
    "    assert len(all_probs) == len(df_new), \n",
    "    \n",
    "    df_new[\"pred_score\"] = all_probs\n",
    "    df_new.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved to {output_csv}. Final predictions: {len(all_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 10153. We will process 51 chunks (chunk_size=200).\n",
      "Chunk 1/51 done. Current total predictions=200\n",
      "Chunk 2/51 done. Current total predictions=400\n",
      "Chunk 3/51 done. Current total predictions=600\n",
      "Chunk 4/51 done. Current total predictions=800\n",
      "Chunk 5/51 done. Current total predictions=1000\n",
      "Chunk 6/51 done. Current total predictions=1200\n",
      "Chunk 7/51 done. Current total predictions=1400\n",
      "Chunk 8/51 done. Current total predictions=1600\n",
      "Chunk 9/51 done. Current total predictions=1800\n",
      "Chunk 10/51 done. Current total predictions=2000\n",
      "Chunk 11/51 done. Current total predictions=2200\n",
      "Chunk 12/51 done. Current total predictions=2400\n",
      "Chunk 13/51 done. Current total predictions=2600\n",
      "Chunk 14/51 done. Current total predictions=2800\n",
      "Chunk 15/51 done. Current total predictions=3000\n",
      "Chunk 16/51 done. Current total predictions=3200\n",
      "Chunk 17/51 done. Current total predictions=3400\n",
      "Chunk 18/51 done. Current total predictions=3600\n",
      "Chunk 19/51 done. Current total predictions=3800\n",
      "Chunk 20/51 done. Current total predictions=4000\n",
      "Chunk 21/51 done. Current total predictions=4200\n",
      "Chunk 22/51 done. Current total predictions=4400\n",
      "Chunk 23/51 done. Current total predictions=4600\n",
      "Chunk 24/51 done. Current total predictions=4800\n",
      "Chunk 25/51 done. Current total predictions=5000\n",
      "Chunk 26/51 done. Current total predictions=5200\n",
      "Chunk 27/51 done. Current total predictions=5400\n",
      "Chunk 28/51 done. Current total predictions=5600\n",
      "Chunk 29/51 done. Current total predictions=5800\n",
      "Chunk 30/51 done. Current total predictions=6000\n",
      "Chunk 31/51 done. Current total predictions=6200\n",
      "Chunk 32/51 done. Current total predictions=6400\n",
      "Chunk 33/51 done. Current total predictions=6600\n",
      "Chunk 34/51 done. Current total predictions=6800\n",
      "Chunk 35/51 done. Current total predictions=7000\n",
      "Chunk 36/51 done. Current total predictions=7200\n",
      "Chunk 37/51 done. Current total predictions=7400\n",
      "Chunk 38/51 done. Current total predictions=7600\n",
      "Chunk 39/51 done. Current total predictions=7800\n",
      "Chunk 40/51 done. Current total predictions=8000\n",
      "Chunk 41/51 done. Current total predictions=8200\n",
      "Chunk 42/51 done. Current total predictions=8400\n",
      "Chunk 43/51 done. Current total predictions=8600\n",
      "Chunk 44/51 done. Current total predictions=8800\n",
      "Chunk 45/51 done. Current total predictions=9000\n",
      "Chunk 46/51 done. Current total predictions=9200\n",
      "Chunk 47/51 done. Current total predictions=9400\n",
      "Chunk 48/51 done. Current total predictions=9600\n",
      "Chunk 49/51 done. Current total predictions=9800\n",
      "Chunk 50/51 done. Current total predictions=10000\n",
      "Chunk 51/51 done. Current total predictions=10153\n",
      "Saved to ../Gerneral_db_pred_result.csv. Final predictions: 10153\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "predict_on_csv_in_chunks(\n",
    "    csv_file=\"../microprotein_db_seq.csv\",\n",
    "    model_file=\"best_mlp.pth\",\n",
    "    device=device,\n",
    "    output_csv=\"../microprotein_db_predicted_stability.csv\",\n",
    "    chunk_size=200\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESMfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
